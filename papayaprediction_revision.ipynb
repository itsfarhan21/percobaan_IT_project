{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5HA_JMdif3h"
      },
      "outputs": [],
      "source": [
        "#import modul-modul untuk membantu jalannya program\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fNCgrhgQkpQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mmenyambungkan google collab dengan Google Drive untuk membaca dataset\n",
        "from google.colab import drive #dari google collab maka kita import google drive \n",
        "drive.mount(\"/content/MyDrive\") #menggunakan fungsi drive.mount untuk mengakses google drive dengan akun google yang dikaitkan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdy7gjjKiglY",
        "outputId": "037fd7a5-a1e0-4a8b-dd09-ec1a045526df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#membaca dataset melalui Google Drive dan mengekstrak dataset \n",
        "local_zip = '/content/MyDrive/MyDrive/datasetpepaya.zip' #panggildataset dengan directory /content/myDrive/MyDrive/namadataset\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r') #fungsi \"r\", untuk membaca file yang berada di dalam zip\n",
        "zip_ref.extractall(\"/tmp\") #file zip diekstrak\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "o37clOcyigoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#memishakan data training dan data testing\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "\n",
        "  files=[] #membuat sebuah list kosong dengan nama variabel file\n",
        "  for filename in os.listdir(SOURCE): #file yang berada di dalam operasi sistem\n",
        "    file = SOURCE + filename #variabel file adalah sumber + namafile yang telah diekstrak\n",
        "    if os.path.getsize(file) > 0: #jika ukuran file > 0, maka file dinyatakan tersedia, sedangkan jika < 0 atau = 0, maka file tersebut tidak memiliki nilai dan isi (kosong)\n",
        "      files.append(filename)\n",
        "    else:\n",
        "      print('File is empty')\n",
        "\n",
        "    #mekanisme training dan testing \n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[-testing_length:]\n",
        "  \n",
        "  for filename in training_set: #nama file digunakan untuk training\n",
        "    src_file = SOURCE + filename\n",
        "    dest_file = TRAINING + filename\n",
        "    copyfile(src_file, dest_file)\n",
        "    \n",
        "  for filename in testing_set: #nama file yang digunakan untuk testing\n",
        "    src_file = SOURCE + filename\n",
        "    dest_file = TESTING + filename\n",
        "    copyfile(src_file, dest_file)"
      ],
      "metadata": {
        "id": "UZT237tIigqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model convolutional neural network (CNN)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16,(3,3), activation = 'relu', input_shape=(150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32,(3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64,(3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "TRAINING_DIR = '/tmp/papaya_image/' #memanggil file yang telah diekstrak untuk di generate yang berguna untuk keperluan training data\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                         batch_size=100,\n",
        "                                                         class_mode='categorical', \n",
        "                                                         target_size=(150,150))\n",
        "\n",
        "VALIDATION_DIR = '/tmp/papaya_image/' #memanggil file yang telah diekstrak untuk di genearte yang berguna untuk keperluan testing data\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                         batch_size=100,\n",
        "                                                         class_mode='categorical', \n",
        "                                                         target_size=(150,150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDEZhHv_oVWg",
        "outputId": "02241689-d677-4403-dec9-02cbfd88d2b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 1 classes.\n",
            "Found 300 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#menampilkan histori dan hasil dari preprocessing data menggunakan model Convolutional Neural Network (CNN) \n",
        "history = model.fit(train_generator,\n",
        "                              epochs=25,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi7KXRwKoVZD",
        "outputId": "f19edd20-cf39-47b0-a2bb-abe72e9ee5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "3/3 [==============================] - 59s 23s/step - loss: 1.9176e-08 - acc: 1.0000 - val_loss: 1.8565e-08 - val_acc: 1.0000\n",
            "Epoch 2/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 1.8275e-08 - acc: 1.0000 - val_loss: 1.7602e-08 - val_acc: 1.0000\n",
            "Epoch 3/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.7248e-08 - acc: 1.0000 - val_loss: 1.6570e-08 - val_acc: 1.0000\n",
            "Epoch 4/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.6213e-08 - acc: 1.0000 - val_loss: 1.5466e-08 - val_acc: 1.0000\n",
            "Epoch 5/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 1.5072e-08 - acc: 1.0000 - val_loss: 1.4296e-08 - val_acc: 1.0000\n",
            "Epoch 6/25\n",
            "3/3 [==============================] - 49s 19s/step - loss: 1.3853e-08 - acc: 1.0000 - val_loss: 1.3213e-08 - val_acc: 1.0000\n",
            "Epoch 7/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.2963e-08 - acc: 1.0000 - val_loss: 1.2512e-08 - val_acc: 1.0000\n",
            "Epoch 8/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.2259e-08 - acc: 1.0000 - val_loss: 1.1753e-08 - val_acc: 1.0000\n",
            "Epoch 9/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.1501e-08 - acc: 1.0000 - val_loss: 1.1024e-08 - val_acc: 1.0000\n",
            "Epoch 10/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 1.0810e-08 - acc: 1.0000 - val_loss: 1.0393e-08 - val_acc: 1.0000\n",
            "Epoch 11/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 1.0149e-08 - acc: 1.0000 - val_loss: 9.7196e-09 - val_acc: 1.0000\n",
            "Epoch 12/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 9.4660e-09 - acc: 1.0000 - val_loss: 9.0005e-09 - val_acc: 1.0000\n",
            "Epoch 13/25\n",
            "3/3 [==============================] - 50s 19s/step - loss: 8.7909e-09 - acc: 1.0000 - val_loss: 8.3903e-09 - val_acc: 1.0000\n",
            "Epoch 14/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 8.1891e-09 - acc: 1.0000 - val_loss: 7.7432e-09 - val_acc: 1.0000\n",
            "Epoch 15/25\n",
            "3/3 [==============================] - 49s 19s/step - loss: 7.4796e-09 - acc: 1.0000 - val_loss: 7.0684e-09 - val_acc: 1.0000\n",
            "Epoch 16/25\n",
            "3/3 [==============================] - 50s 19s/step - loss: 6.9510e-09 - acc: 1.0000 - val_loss: 6.7193e-09 - val_acc: 1.0000\n",
            "Epoch 17/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 6.5812e-09 - acc: 1.0000 - val_loss: 6.3388e-09 - val_acc: 1.0000\n",
            "Epoch 18/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 6.2393e-09 - acc: 1.0000 - val_loss: 5.9238e-09 - val_acc: 1.0000\n",
            "Epoch 19/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 5.7629e-09 - acc: 1.0000 - val_loss: 5.4935e-09 - val_acc: 1.0000\n",
            "Epoch 20/25\n",
            "3/3 [==============================] - 49s 19s/step - loss: 5.3750e-09 - acc: 1.0000 - val_loss: 5.0381e-09 - val_acc: 1.0000\n",
            "Epoch 21/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 4.8770e-09 - acc: 1.0000 - val_loss: 4.5740e-09 - val_acc: 1.0000\n",
            "Epoch 22/25\n",
            "3/3 [==============================] - 49s 19s/step - loss: 4.4365e-09 - acc: 1.0000 - val_loss: 4.0905e-09 - val_acc: 1.0000\n",
            "Epoch 23/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 3.9084e-09 - acc: 1.0000 - val_loss: 3.6134e-09 - val_acc: 1.0000\n",
            "Epoch 24/25\n",
            "3/3 [==============================] - 50s 20s/step - loss: 3.5316e-09 - acc: 1.0000 - val_loss: 3.4927e-09 - val_acc: 1.0000\n",
            "Epoch 25/25\n",
            "3/3 [==============================] - 48s 19s/step - loss: 3.4927e-09 - acc: 1.0000 - val_loss: 3.4927e-09 - val_acc: 1.0000\n"
          ]
        }
      ]
    }
  ]
}